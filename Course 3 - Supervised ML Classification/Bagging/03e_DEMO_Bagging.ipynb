{"cells":[{"cell_type":"markdown","metadata":{"run_control":{"marked":true}},"source":["\u003cp style=\"text-align:center\"\u003e\n","    \u003ca href=\"https://skills.network/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML241ENSkillsNetwork820-2023-01-01\"\u003e\n","    \u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  /\u003e\n","    \u003c/a\u003e\n","\u003c/p\u003e\n","\n","# Machine Learning Foundation\n","\n","## Course 3, Part e: Bagging DEMO\n","\n","Estimated time needed: **45** minutes\n","\n","## Objectives\n","\n","After completing this demo you will be able to:\n","\n","*   Understand what Bagging is\n","*   Recognize that Random Forests is an implementation of bagging and its abilities\n","*   Describe the advantages of Random Forests over simply adding extra Decision Trees\n"]},{"cell_type":"markdown","metadata":{},"source":["\u003ch1\u003eTable of contents\u003c/h1\u003e\n","\n","\u003cdiv class=\"alert alert-block alert-info\" style=\"margin-top: 20px\"\u003e\n","    \u003col\u003e\n","        \u003cli\u003e\u003ca href=\"https://#Setup\"\u003ePart 1: Setup\u003c/a\u003e\u003c/li\u003e\n","        \u003cli\u003e\u003ca href=\"https://#TargPreproc\"\u003ePart 2: Examining the Target and Preprocessing\u003c/a\u003e\u003c/li\u003e\n","        \u003cli\u003e\u003ca href=\"https://#OOB\"\u003ePart 3: Random Forest and Out-of-bag Error\u003c/a\u003e\u003c/li\u003e\n","        \u003cli\u003e\u003ca href=\"https://#ExtraTrees\"\u003ePart 4: Extra Trees\u003c/a\u003e\u003c/li\u003e\n","        \u003cli\u003e\u003ca href=\"https://#results\"\u003ePart 5: Gathering Results\u003c/a\u003e\u003c/li\u003e\n","        \u003cli\u003e\u003ca href=\"https://#examineRes\"\u003ePart 6: Examining Results\u003c/a\u003e\u003c/li\u003e\n","\u003c/div\u003e\n","\u003cbr\u003e\n","\u003chr\u003e\n"]},{"cell_type":"markdown","metadata":{},"source":["## Introduction\n","\n","We will be using the customer churn data from the telecom industry that we used in the KNN Lab. Since we preprocessed the data there, we will import the preprocessed data, which is in a file called: 'churndata_processed.csv'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-08-05T08:06:13.025737Z","start_time":"2017-08-05T08:06:13.018886Z"}},"outputs":[],"source":["def warn(*args, **kwargs):\n","    pass\n","import warnings\n","warnings.warn = warn\n","\n","import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns"]},{"cell_type":"markdown","metadata":{},"source":["\u003ch2 id = \"Setup\"\u003e Part 1: Setup \u003c/h2\u003e\n","\n","*   The raw churndata has been setup as a variable 'churndata', and we have imported it above.\n","*   We will rely on the data preprocessing from the KNN lab, which is captured in the file 'churndata_processed.csv'\n","*   First, import that file and examine its contents.\n","*   Output summary statistics and check variable data types\n","*   Using Seaborn, plot a heatmap of variable correlations\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-08-05T08:06:13.728991Z","start_time":"2017-08-05T08:06:13.030365Z"}},"outputs":[],"source":["data = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML241EN-SkillsNetwork/labs/datasets/churndata_processed.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-08-05T08:06:13.764603Z","start_time":"2017-08-05T08:06:13.730908Z"}},"outputs":[],"source":["round(data.describe().T, 2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(15,10)) \n","sns.heatmap(data.corr())"]},{"cell_type":"markdown","metadata":{},"source":["\u003ch2 id=\"TargPreproc\"\u003ePart 2: Examining the Target and Preprocessing\u003c/h2\u003e\n","\n","In this exercise, we will proceed as follows:\n","\n","*   Examine distribution of the predicted variable (`churn_value`).\n","*   Split the data into train and test sets. Decide if a stratified split should be used or not based on the distribution.\n","*   Examine the distribution of the predictor variable in the train and test data.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-08-05T08:06:13.823706Z","start_time":"2017-08-05T08:06:13.815353Z"}},"outputs":[],"source":["# Data are skewed at ~85% towards non-churned customers\n","# This will be important to remember when model building\n","target = 'churn_value'\n","data[target].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-08-05T08:06:13.834394Z","start_time":"2017-08-05T08:06:13.826015Z"}},"outputs":[],"source":["data[target].value_counts(normalize=True)"]},{"cell_type":"markdown","metadata":{},"source":["Given the skew in the predictor variable, let's split the data with the *churned* values being stratified.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-08-05T08:06:14.811974Z","start_time":"2017-08-05T08:06:13.836891Z"}},"outputs":[],"source":["from sklearn.model_selection import StratifiedShuffleSplit\n","\n","\n","feature_cols = [x for x in data.columns if x != target]\n","\n","\n","# Split the data into two parts with 1500 points in the test data\n","# This creates a generator\n","strat_shuff_split = StratifiedShuffleSplit(n_splits=1, test_size=1500, random_state=42)\n","\n","# Get the index values from the generator\n","train_idx, test_idx = next(strat_shuff_split.split(data[feature_cols], data[target]))\n","\n","# Create the data sets\n","X_train = data.loc[train_idx, feature_cols]\n","y_train = data.loc[train_idx, target]\n","\n","X_test = data.loc[test_idx, feature_cols]\n","y_test = data.loc[test_idx, target]"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-08-05T08:06:14.823171Z","start_time":"2017-08-05T08:06:14.814134Z"}},"outputs":[],"source":["y_train.value_counts(normalize=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-08-05T08:06:14.833664Z","start_time":"2017-08-05T08:06:14.825182Z"}},"outputs":[],"source":["y_test.value_counts(normalize=True)"]},{"cell_type":"markdown","metadata":{},"source":["\u003ch2 id =\"OOB\"\u003ePart 3: Random Forest and Out-of-bag Error\u003c/h2\u003e\n","\n","In this exercise, we will:\n","\n","*   Fit random forest models with a range of tree numbers and evaluate the out-of-bag error for each of these models.\n","*   Plot the resulting oob errors as a function of the number of trees.\n","\n","*Note:* since the only thing changing is the number of trees, the `warm_start` flag can be used so that the model just adds more trees to the existing model each time. Use the `set_params` method to update the number of trees.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-08-05T08:06:14.841222Z","start_time":"2017-08-05T08:06:14.835671Z"}},"outputs":[],"source":["# Suppress warnings about too few trees from the early models\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-08-05T08:06:17.513569Z","start_time":"2017-08-05T08:06:14.843363Z"}},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","# Initialize the random forest estimator\n","# Note that the number of trees is not setup here\n","RF = RandomForestClassifier(oob_score=True, \n","                            random_state=42, \n","                            warm_start=True,\n","                            n_jobs=-1)\n","\n","oob_list = list()\n","\n","# Iterate through all of the possibilities for \n","# number of trees\n","for n_trees in [15, 20, 30, 40, 50, 100, 150, 200, 300, 400]:\n","    \n","    # Use this to set the number of trees\n","    RF.set_params(n_estimators=n_trees)\n","\n","    # Fit the model\n","    RF.fit(X_train, y_train)\n","\n","    # Get the oob error\n","    oob_error = 1 - RF.oob_score_\n","    \n","    # Store it\n","    oob_list.append(pd.Series({'n_trees': n_trees, 'oob': oob_error}))\n","\n","rf_oob_df = pd.concat(oob_list, axis=1).T.set_index('n_trees')\n","\n","rf_oob_df"]},{"cell_type":"markdown","metadata":{},"source":["The error looks like it has stabilized around 100-150 trees.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-08-05T08:06:17.828505Z","start_time":"2017-08-05T08:06:17.515606Z"}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-08-05T08:06:18.140058Z","start_time":"2017-08-05T08:06:17.830038Z"}},"outputs":[],"source":["sns.set_context('talk')\n","sns.set_style('white')\n","\n","ax = rf_oob_df.plot(legend=False, marker='o', figsize=(14, 7), linewidth=5)\n","ax.set(ylabel='out-of-bag error');"]},{"cell_type":"markdown","metadata":{},"source":["\u003ch2 id = \"ExtraTrees\"\u003ePart 4: Extra Trees\u003c/h2\u003e\n","\n","Our exercise:\n","\n","*   Repeat question 3 using extra randomized trees (`ExtraTreesClassifier`). Note that the `bootstrap` parameter will have to be set to `True` for this model.\n","*   Compare the out-of-bag errors for the two different types of models.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-08-05T08:06:19.957155Z","start_time":"2017-08-05T08:06:18.141448Z"}},"outputs":[],"source":["from sklearn.ensemble import ExtraTreesClassifier\n","\n","# Initialize the random forest estimator\n","# Note that the number of trees is not setup here\n","EF = ExtraTreesClassifier(oob_score=True, \n","                          random_state=42, \n","                          warm_start=True,\n","                          bootstrap=True,\n","                          n_jobs=-1)\n","\n","oob_list = list()\n","\n","# Iterate through all of the possibilities for \n","# number of trees\n","for n_trees in [15, 20, 30, 40, 50, 100, 150, 200, 300, 400]:\n","    \n","    # Use this to set the number of trees\n","    EF.set_params(n_estimators=n_trees)\n","    EF.fit(X_train, y_train)\n","\n","    # oob error\n","    oob_error = 1 - EF.oob_score_\n","    oob_list.append(pd.Series({'n_trees': n_trees, 'oob': oob_error}))\n","\n","et_oob_df = pd.concat(oob_list, axis=1).T.set_index('n_trees')\n","\n","et_oob_df"]},{"cell_type":"markdown","metadata":{},"source":["Combine the two dataframes into a single one for easier plotting.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-08-05T08:06:19.974338Z","start_time":"2017-08-05T08:06:19.958994Z"}},"outputs":[],"source":["oob_df = pd.concat([rf_oob_df.rename(columns={'oob':'RandomForest'}),\n","                    et_oob_df.rename(columns={'oob':'ExtraTrees'})], axis=1)\n","\n","oob_df"]},{"cell_type":"markdown","metadata":{},"source":["The random forest model performs consistently better than the extra randomized trees.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-08-05T08:06:20.258026Z","start_time":"2017-08-05T08:06:19.976774Z"}},"outputs":[],"source":["sns.set_context('talk')\n","sns.set_style('white')\n","\n","ax = oob_df.plot(marker='o', figsize=(14, 7), linewidth=5)\n","ax.set(ylabel='out-of-bag error');"]},{"cell_type":"markdown","metadata":{},"source":["\u003ch2 id=\"results\"\u003ePart 5: Gathering Results\u003c/h2\u003e\n","\n","Here, we will:\n","\n","*   Select one of the models that performs well and calculate error metrics and a confusion matrix on the test data set.\n","*   Given the distribution of the predicted class, which metric is most important? Which could be deceiving?\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-08-05T08:06:20.468575Z","start_time":"2017-08-05T08:06:20.260111Z"}},"outputs":[],"source":["# Random forest with 100 estimators\n","model = RF.set_params(n_estimators=100)\n","\n","y_pred = model.predict(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["Unsurprisingly, recall is rather poor for the customers who churned (True) class since they are quite small. We are doing better than random guessing, though, as the accuracy is 0.96 (vs 0.85 for random guessing).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-08-05T08:06:20.495246Z","start_time":"2017-08-05T08:06:20.470532Z"}},"outputs":[],"source":["from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score\n","from sklearn.metrics import f1_score, roc_auc_score\n","\n","cr = classification_report(y_test, y_pred)\n","print(cr)\n","\n","score_df = pd.DataFrame({'accuracy': accuracy_score(y_test, y_pred),\n","                         'precision': precision_score(y_test, y_pred),\n","                         'recall': recall_score(y_test, y_pred),\n","                         'f1': f1_score(y_test, y_pred),\n","                         'auc': roc_auc_score(y_test, y_pred)},\n","                         index=pd.Index([0]))\n","\n","print(score_df)"]},{"cell_type":"markdown","metadata":{},"source":["\u003ch2 id=\"examineRes\"\u003ePart 6: Examining Results\u003c/h2\u003e\n","\n","The following exercises will help us examine results:\n","\n","*   Print or visualize the confusion matrix.\n","*   Plot the ROC-AUC and precision-recall curves.\n","*   Plot the feature importances.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-08-05T08:06:20.799496Z","start_time":"2017-08-05T08:06:20.497902Z"}},"outputs":[],"source":["from sklearn.metrics import roc_curve, precision_recall_curve, confusion_matrix\n","\n","sns.set_context('talk')\n","cm = confusion_matrix(y_test, y_pred)\n","_, ax = plt.subplots(figsize=(12,12))\n","ax = sns.heatmap(cm, annot=True, fmt='d', annot_kws={\"size\": 40, \"weight\": \"bold\"})\n","\n","labels = ['False', 'True']\n","ax.set_xticklabels(labels, fontsize=25);\n","ax.set_yticklabels(labels[::-1], fontsize=25);\n","ax.set_ylabel('Prediction', fontsize=30);\n","ax.set_xlabel('Ground Truth', fontsize=30)"]},{"cell_type":"markdown","metadata":{},"source":["The ROC-AUC and precision-recall curves.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-08-05T08:06:21.375930Z","start_time":"2017-08-05T08:06:20.800960Z"}},"outputs":[],"source":["sns.set_context('talk')\n","\n","fig, axList = plt.subplots(ncols=2)\n","fig.set_size_inches(16, 8)\n","\n","# Get the probabilities for each of the two categories\n","y_prob = model.predict_proba(X_test)\n","\n","# Plot the ROC-AUC curve\n","ax = axList[0]\n","\n","fpr, tpr, thresholds = roc_curve(y_test, y_prob[:,1])\n","ax.plot(fpr, tpr, linewidth=5)\n","# It is customary to draw a diagonal dotted line in ROC plots.\n","# This is to indicate completely random prediction. Deviation from this\n","# dotted line towards the upper left corner signifies the power of the model.\n","ax.plot([0, 1], [0, 1], ls='--', color='black', lw=.3)\n","ax.set(xlabel='False Positive Rate',\n","       ylabel='True Positive Rate',\n","       xlim=[-.01, 1.01], ylim=[-.01, 1.01],\n","       title='ROC curve')\n","ax.grid(True)\n","\n","# Plot the precision-recall curve\n","ax = axList[1]\n","\n","precision, recall, _ = precision_recall_curve(y_test, y_prob[:,1])\n","ax.plot(recall, precision, linewidth=5)\n","ax.set(xlabel='Recall', ylabel='Precision',\n","       xlim=[-.01, 1.01], ylim=[-.01, 1.01],\n","       title='Precision-Recall curve')\n","ax.grid(True)\n","\n","plt.tight_layout()"]},{"cell_type":"markdown","metadata":{},"source":["The feature importances. Total daily cost is the biggest predictor of customer churn.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-08-05T08:06:21.892647Z","start_time":"2017-08-05T08:06:21.377664Z"}},"outputs":[],"source":["feature_imp = pd.Series(model.feature_importances_, index=feature_cols).sort_values(ascending=False)\n","\n","ax = feature_imp.plot(kind='bar', figsize=(16, 6))\n","ax.set(ylabel='Relative Importance');\n","ax.set(ylabel='Feature');"]},{"cell_type":"markdown","metadata":{},"source":["### Thank you for completing this lab!\n","\n","## Author\n","\n","\u003ca href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2021-01-01\"\u003eJoseph Santarcangelo\u003c/a\u003e\n","\n","### Other Contributors\n","\n","\u003ca href=\"https://www.linkedin.com/in/richard-ye/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2021-01-01\"\u003eRichard Ye\u003c/a\u003e\n","\n","## Change Log\n","\n","| Date (YYYY-MM-DD) | Version | Changed By | Change Description |\n","| ----------------- | ------- | ---------- | ------------------ |\n","| 2022-01-27        | 0.1     | Joseph Santarcangelo | Created Lab Template |\n","| 2022-05-02        | 0.2     | Richard Ye | Added in estimated time, objectives and table of contents |\n","\n","### Machine Learning Foundation (C) 2020 IBM Corporation\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}