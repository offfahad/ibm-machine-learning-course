{"cells":[{"cell_type":"markdown","metadata":{},"source":["\u003cp style=\"text-align:center\"\u003e\n","    \u003ca href=\"https://skills.network/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML241ENSkillsNetwork820-2023-01-01\"\u003e\n","    \u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  /\u003e\n","    \u003c/a\u003e\n","\u003c/p\u003e\n"]},{"cell_type":"markdown","metadata":{},"source":["# **K Nearest Neighbor**\n"]},{"cell_type":"markdown","metadata":{},"source":["Estimated time needed: **30** minutes\n"]},{"cell_type":"markdown","metadata":{},"source":["In this lab, you will learn about and practice the K Nearest Neighbor (KNN) model. KNN is a straightforward but very effective model that can be used for both classification and regression tasks. If the feature space is not very large, KNN can be a high-interpretable model because you can explain and understand how a prediction is made by looking at its nearest neighbors.\n"]},{"cell_type":"markdown","metadata":{},"source":["We will be using a tumor sample dataset containing lab test results about tumor samples. The objective is to classify whether a tumor is malicious (cancer) or benign. As such, it is a typical binary classification task.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Objectives\n"]},{"cell_type":"markdown","metadata":{},"source":["After completing this lab, you will be able to:\n"]},{"cell_type":"markdown","metadata":{},"source":["* Train KNN models with different neighbor hyper-parameters\n","* Evaluate KNN models on classification tasks\n","* Tune the number of neighbors and find the optimized one for a specific task\n"]},{"cell_type":"markdown","metadata":{},"source":["----\n"]},{"cell_type":"markdown","metadata":{},"source":["First, let's install `seaborn` for visualization tasks and import required libraries for this lab.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n","# !mamba install -qy pandas==1.3.3 numpy==1.21.2 ipywidgets==7.4.2 scipy==7.4.2 tqdm==4.62.3 matplotlib==3.5.0 seaborn==0.9.0\n","# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\"."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","# Evaluation metrics related methods\n","from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_recall_fscore_support, precision_score, recall_score\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define a random seed to reproduce any random process\n","rs = 123"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Ignore any deprecation warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "]},{"cell_type":"markdown","metadata":{},"source":["## Load and explore the tumor sample dataset\n"]},{"cell_type":"markdown","metadata":{},"source":["We first load the dataset `tumor.csv` as a Pandas dataframe:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Read datast in csv format\n","dataset_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML241EN-SkillsNetwork/labs/datasets/tumor.csv\"\n","tumor_df = pd.read_csv(dataset_url)"]},{"cell_type":"markdown","metadata":{},"source":["Then, let's quickly take a look at the head of the dataframe.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tumor_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["And, display its columns.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tumor_df.columns"]},{"cell_type":"markdown","metadata":{},"source":["Each observation in this dataset contains lab test results about a tumor sample, such as clump or shapes. Based on these lab test results or features, we want to build a classification model to predict if this tumor sample is malicious (cancer) or benign. The target variable `y` is specified in the `Class` column.\n"]},{"cell_type":"markdown","metadata":{},"source":["Then, let's split the dataset into input `X` and output `y`:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X = tumor_df.iloc[:, :-1]\n","y = tumor_df.iloc[:, -1:]"]},{"cell_type":"markdown","metadata":{},"source":["And, we first check the statistics summary of features in `X`:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X.describe()"]},{"cell_type":"markdown","metadata":{},"source":["As we can see from the above cell output, all features are numeric and ranged between 1 to 10. This is very convenient as we do not need to scale the feature values as they are already in the same range.\n"]},{"cell_type":"markdown","metadata":{},"source":["Next, let's check the class distribution of output `y`:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y.value_counts(normalize=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y.value_counts().plot.bar(color=['green', 'red'])"]},{"cell_type":"markdown","metadata":{},"source":["We have about 65% benign tumors (`Class = 0`) and 35% cancerous tumors (`Class = 1`), which is not a very imbalanced class distribution. \n"]},{"cell_type":"markdown","metadata":{},"source":["## Process and split training and testing datasets\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Split 80% as training dataset\n","# and 20% as testing dataset\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state = rs)"]},{"cell_type":"markdown","metadata":{},"source":["## Train and evaluate a KNN classifier with the number of neighbors set to 2\n"]},{"cell_type":"markdown","metadata":{},"source":["Training a KNN classifier is very similar to training other classifiers in `sklearn`, we first need to define a `KNeighborsClassifier` object. Here we use `n_neighbors=2` argument to specify how many neighbors will be used for prediction, and we keep other arguments to be their default values.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define a KNN classifier with `n_neighbors=2`\n","knn_model = KNeighborsClassifier(n_neighbors=2)"]},{"cell_type":"markdown","metadata":{},"source":["Then we can train the model with `X_train` and `y_train`, and we use ravel() method to convert the data frame `y_train` to a vector.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["knn_model.fit(X_train, y_train.values.ravel())"]},{"cell_type":"markdown","metadata":{},"source":["And, we can make predictions on the `X_test` dataframe.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["preds = knn_model.predict(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["To evaluate the KNN classifier, we provide a pre-defined method to return the commonly used evaluation metrics such as accuracy, recall, precision, f1score, and so on, based on the true classes in the 'y_test' and model predictions.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def evaluate_metrics(yt, yp):\n","    results_pos = {}\n","    results_pos['accuracy'] = accuracy_score(yt, yp)\n","    precision, recall, f_beta, _ = precision_recall_fscore_support(yt, yp, average='binary')\n","    results_pos['recall'] = recall\n","    results_pos['precision'] = precision\n","    results_pos['f1score'] = f_beta\n","    return results_pos"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["evaluate_metrics(y_test, preds)"]},{"cell_type":"markdown","metadata":{},"source":["We can see that there is a great classification performance on the tumor sample dataset. This means the KNN model can effectively recognize cancerous tumors.\n","Next, it's your turn to try a different number of neighbors to see if we could get even better performance.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Coding exercise: Train and evaluate a KNN classifier with number of neighbors set to 5\n"]},{"cell_type":"markdown","metadata":{},"source":["First, define a KNN classifier with KNeighborsClassifier class:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Type your code here\n"]},{"cell_type":"markdown","metadata":{},"source":["Then train the model with `X_train` and `y_train`:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Type your code here\n"]},{"cell_type":"markdown","metadata":{},"source":["And, make predictions on `X_test` dataframe:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Type your code here\n","model = KNeighborsClassifier(n_neighbors=5)\n","model.fit(X_train, y_train.values.ravel())\n","preds = model.predict(X_test)\n","evaluate_metrics(y_test, preds)"]},{"cell_type":"markdown","metadata":{},"source":["At last, you can evaluate your KNN model with provided `evaluate_metrics()` method.\n"]},{"cell_type":"markdown","metadata":{},"source":["\u003cdetails\u003e\u003csummary\u003eClick here for a sample solution\u003c/summary\u003e\n","\n","```python\n","model = KNeighborsClassifier(n_neighbors=5)\n","model.fit(X_train, y_train.values.ravel())\n","preds = model.predict(X_test)\n","evaluate_metrics(y_test, preds)\n","```\n","\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","metadata":{},"source":["## Tune the number of neighbors to find the optmized one\n"]},{"cell_type":"markdown","metadata":{},"source":["OK, you may wonder which `n_neighbors` argument may give you the best classification performance. We can try different `n_neighbors` (the K value) and check which `K` gives the best classification performance.\n"]},{"cell_type":"markdown","metadata":{},"source":["Here we could try K from 1 to 50, and store the aggregated `f1score` for each k into a list.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Try K from 1 to 50\n","max_k = 50\n","# Create an empty list to store f1score for each k\n","f1_scores = []"]},{"cell_type":"markdown","metadata":{},"source":["Then we will train 50 KNN classifiers with K ranged from 1 to 50.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for k in range(1, max_k + 1):\n","    # Create a KNN classifier\n","    knn = KNeighborsClassifier(n_neighbors=k)\n","    # Train the classifier\n","    knn = knn.fit(X_train, y_train.values.ravel())\n","    preds = knn.predict(X_test)\n","    # Evaluate the classifier with f1score\n","    f1 = f1_score(preds, y_test)\n","    f1_scores.append((k, round(f1_score(y_test, preds), 4)))\n","# Convert the f1score list to a dataframe\n","f1_results = pd.DataFrame(f1_scores, columns=['K', 'F1 Score'])\n","f1_results.set_index('K')"]},{"cell_type":"markdown","metadata":{},"source":["This is a long list and different to analysis, so let's visualize the list using a linechart.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot F1 results\n","ax = f1_results.plot(figsize=(12, 12))\n","ax.set(xlabel='Num of Neighbors', ylabel='F1 Score')\n","ax.set_xticks(range(1, max_k, 2));\n","plt.ylim((0.85, 1))\n","plt.title('KNN F1 Score')"]},{"cell_type":"markdown","metadata":{},"source":["As we can see from the F1 score linechart, the best `K` value is 5 with about `0.9691` f1score.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Next steps\n"]},{"cell_type":"markdown","metadata":{},"source":["Great! Now you have learned about and applied the KNN model to solve a real-world tumor type classification problem. You also tuned the KNN to find the best K value. Later, you will continue learning other popular classification models with different structures, assumptions, cost functions, and application scenarios.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Authors\n"]},{"cell_type":"markdown","metadata":{},"source":["[Yan Luo](https://www.linkedin.com/in/yan-luo-96288783/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML241ENSkillsNetwork820-2023-01-01)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Other Contributors\n"]},{"cell_type":"markdown","metadata":{},"source":["## Change Log\n"]},{"cell_type":"markdown","metadata":{},"source":["|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n","|-|-|-|-|\n","|2021-11-9|1.0|Yan|Created the initial version|\n","|2022-3-29|1.1|Steve Hord|QA Pass|\n"]},{"cell_type":"markdown","metadata":{},"source":["Copyright Â© 2021 IBM Corporation. All rights reserved.\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}